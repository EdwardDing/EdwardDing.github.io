---
layout: post
title: "Machine Learning 4"
categories: machine-learning
excerpt: Generative Learning Algorithms, GDA, Naive Bayes and Laplave smoothing.
---

{% include custom/mathjax.html %}

###Generative Learning Algorithm

-----------------------
All classification algorithms we've been learned so far are called `Discrimitive Learning Algorithms`, in which we learn \\( p(y \mid x) \\) or \\( h_\theta(x) \\) directly. Now, we gonna learn a new kind of method called `Generative Learning Algorithms`, in which we learn \\( p(x \mid y) \\) and \\( p(y) \\) first, and then compute \\( p(y \mid x) \\) using Bayes formula.

$$
p(y=1 \mid x) = \frac{p(x \mid y=1)p(y=1)}{p(x)}
$$

For example, say, we want to build a model to classify dog and cat. By using discrimitive model, we build one model that learn features from both so that it can discriminate between dog and cat directly. We simply input an image and get the answer. While, by using generative model, we generate two models, one for the dog and one for the cat. Then we can classify a new input by comparing two result from those models to see whether it fits dog model better or the other one.

<div style="text-align: center">
	<a href="/assets/machineLearning/doge.png">
		<img src="/assets/machineLearning/doge.png" alt="doge" height="50%" width="50%">
		</img>
	</a>
</div>

###Gaussian Discriminant Analysis

--------------------
Gaussian discriminant analysis (GDA) is a generative model for continous input. We'll assume that \\( p(x \mid y) \\) is distributed accoding to a multivariate normal distribution.

The PMF for a multivariate normal disribution is:

$$
p(z) = \frac{1}{(2\pi)^\frac{1}{n}|\Sigma|^\frac{1}{2}}exp[-\frac{1}{2}(x-\mu)^T \Sigma ^{-1}(x-\mu)]

$$

where 
$$
\Sigma=E[(x-\mu)(x-\mu)^T]
$$
is the `covariance matrix`.

Now we have all we need:

$$
\begin{align}
p(y) &= \phi^y(1-\phi)^{1-y} \\
p(x \mid y=0) &= \frac{1}{(2\pi)^\frac{1}{n}|\Sigma|^\frac{1}{2}}exp[-\frac{1}{2}(x-\mu_0)^T \Sigma ^{-1}(x-\mu_0)] \\
p(x \mid y=1) &= \frac{1}{(2\pi)^\frac{1}{n}|\Sigma|^\frac{1}{2}}exp[-\frac{1}{2}(x-\mu_1)^T \Sigma ^{-1}(x-\mu_1)]
\end{align}
$$

Parameters here are \\( \Sigma, \mu_0, \mu_1 \\) and \\( \phi \\).

As usual we can write down the log likelihood of those parameters.

$$
\begin{align}
l(\phi,\mu_0,\mu_1,\Sigma) 
&= log \prod_{i=1}^{m}p(x^{(i)}, y^{(i)}) \\
&= log \prod_{i=1}^{m}p(x^{(i)} \mid y^{(i)})p(y^{(i)})
\end{align}
$$

By maximizing the log likelihood, we can get the max liakelihood estimate of those paramters:

$$
\begin{align}
\phi &= \frac{\sum y^{(i)}}{m} = \frac{\sum_{i=1}^{m} 1\{ y^{(i)} = 1 \}}{m} \\
\mu_0 &= \frac{\sum_{i=1}^{m} 1\{ y^{(i)} = 0 \} x^{(i)} }{\sum_{i=1}^{m} 1\{ y^{(i)}=0 \}} \\

\mu_1 &= \frac{\sum_{i=1}^{m} 1\{ y^{(i)} = 1 \} x^{(i)} }{\sum_{i=1}^{m} 1\{ y^{(i)}=1 \}} \\

\Sigma &= \frac{1}{m}\sum_{i=1}^{m}(x^{(i)} - \mu_y(i))(x^{(i)} - \mu_y(i))^T
\end{align}
$$

> Note: The formula should make sense. e.g. the numerator of \\( \mu_0 \\) represent the sum of \\( x^{(i)} \\) corresponding to examples where \\( y=0 \\). And the denominator represent the number of examples that output 0.

Also, we can calculate \\( p(x) \\) easily by \\( p(x) = p(x \mid y = 1)p(y=1) + p(x \mid y=0)p(y=0) \\).

With those parameters, we can easily get the distribution of different labels, just like the image shows.

<div style="text-align: center; width:100% ">
	<a href="/assets/machineLearning/GDA.png">
		<img src="/assets/machineLearning/GDA.png" alt="GDA">
		</img>
	</a>
</div>

We can find that the function \\( p(y=1 \mid x) \\) is just like sigmod funtion.

> Note: GDA makes stronger modeling assumptions \\( X \sim N(\mu,\Sigma)\\) and is more data efficient. Logestic regression makes weaker assumptions, but is significantly more robust to deviations from modeling assumptions.

###Naive Bayes Algorithms

--------------------

Naive Bayes Algoritms is another generative algorithm that handle discrete problmes, here we have
$$
y \in \{ 0,1 \}
$$

Let's say, we want to build a filter of spam email. We are using \\( X \\) to represent whether a word in the vocabulary list exists in an email.

$$
X = 
\begin{bmatrix}
1\\ 
0\\ 
1\\ 
\vdots\\ 
0\\ 
\end{bmatrix}
$$

Of course we can use the GLM to solve this, but the problem is we have too many parameters. If we were to model \\( x \\) explicitly with a multinomial distribution, we'd end up with a \\( (2^n - 1) \\) dimentional parameter vector, where n usually are very big.

Thus, we have to make a strong assumption over \\(p(x \mid y)\\). We will assume that the \\( x_i\\)'s are conditionally independent given \\( y \\). This assumption is called the `Naive Bayes Assumption`, the resulting algorithm is called the `Naive Bayes Classifier`. Now we have:

$$
\begin{align}
p(x_1,\cdots,x_n \mid y)
&= p(x_1 \mid y)p(x_2 \mid y,x_1) \cdots p(x_n \mid x_1,x_2,\cdots,x_n-1) \\
&= p(x_1 \mid y)p(x_2 \mid y) \cdots p(x_n \mid y) \\
&= \prod_{i=1}^{n}p(x_i \mid y)
\end{align}
$$

Althogh the assumption itself is false under most cases (e.g. when the word Ding appears, you would porbably find the word Edward in the eamil as well), the algorithm truns out works very well.

The model is parameterized by 
$$
\phi_{i \mid y=1} = p(x_i=1 \mid y=1), \phi_{i \mid y=0} = p(x_i=1 \mid y=0)
$$
and
$$
\phi_y = p(y=1)
$$
. As usual, we can get the estimates of parameters by maximizing the joint log likelihood:

$$
\begin{align}
\phi_{j \mid y=1} 
&= \frac{\sum_{i=1}^{m} 1\{ x_j^{(i)}=1 \wedge  y^{(i)} = 1 \} }{\sum_{i=1}^{m} 1\{ y^{(i)} = 1 \} } \\
\phi_{j \mid y=0} 
&= \frac{\sum_{i=1}^{m} 1\{ x_j^{(i)}=1 \wedge  y^{(i)} = 0 \} }{\sum_{i=1}^{m} 1\{ y^{(i)} = 0 \} } \\
\phi_y 
&= \frac{\sum_{i=1}^{m} 1\{ y^{(i)} = 1 \}}{m}
\end{align}
$$

###Laplace Smoothing

-------------
Actually ther is a deadly problem with the Naive Bayes Algorithm when a new email comes up with a word that is not in your vocabulary. As this is the first time the word appears, 
$$
p(x_{n+1} \mid y=1) = 0, p(x_{n+1} \mid y=0) = 0
$$
. Thus we have
$$
p(y=1 \mid x) = \frac{0}{0 + 0}
$$
which is not defined.

Apparently this is not reasonable, as it seems to say that if you haven't seen a word before, you will never ever see it in the future. This is obviously ridiculous.

Here is the fix, called `Laplace Smoothing`. 

$$
\phi_j = \frac{\sum_{i=1}^{m} 1\{ z^{(i)} = j \} + 1}{m + k}
$$

> Note: \\( \sum_{j=1}^{k} \phi_j = 1 \\) still holds.

Using Laplace smoothing, the \\( \phi_j \\) will never be zero. And our modified `Naive Bayes Classifier` become:

$$
\begin{align}
\phi_{j \mid y=1} 
&= \frac{\sum_{i=1}^{m} 1\{ x_j^{(i)}=1 \wedge  y^{(i)} = 1 \} + 1}{\sum_{i=1}^{m} 1\{ y^{(i)} = 1 \} + 2} \\
\phi_{j \mid y=0} 
&= \frac{\sum_{i=1}^{m} 1\{ x_j^{(i)}=1 \wedge  y^{(i)} = 0 \} + 1}{\sum_{i=1}^{m} 1\{ y^{(i)} = 0 \} + 2} \\
\phi_y 
&= \frac{\sum_{i=1}^{m} 1\{ y^{(i)} = 1 \} + 1}{m + 2}
\end{align}
$$

